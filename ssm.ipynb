{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M74tgUA2GT7I",
        "outputId": "14d544d5-f98b-4839-d87b-ddde48932464"
      },
      "outputs": [],
      "source": [
        "# %pip install transformers\n",
        "import copy\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from torch.utils.data import TensorDataset\n",
        "from sklearn.model_selection import KFold\n",
        "from transformers import BertTokenizer, get_linear_schedule_with_warmup, BertForSequenceClassification, logging\n",
        "logging.set_verbosity_error()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "P4yOcZVWJWap"
      },
      "source": [
        "# **Global Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ny3yIshfJiVb"
      },
      "outputs": [],
      "source": [
        "USE_CV = True # True => Use K-Fold cross validation split | False => Use conventional train/validate/test split\n",
        "SEED = 20 # Global seed value for reproducibility\n",
        "\n",
        "# Conventional data loading settings\n",
        "TRAINING_PORTION = 0.8\n",
        "TESTING_PORTION = 0.2\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "# K-Fold cross validation settings\n",
        "N_SPLITS = int(1/(TESTING_PORTION))\n",
        "KFOLD_SHUFFLE = True\n",
        "\n",
        "\n",
        "PROJECT_DIR = '.'\n",
        "MODEL_NAME = \"SE-BERT\" # bert-base-uncased or dbmdz/bert-base-turkish-uncased or BERT-SE or SE-BERT or SE-BERTurk\n",
        "\n",
        "if MODEL_NAME == 'bert-base-uncased' or MODEL_NAME == 'dbmdz/bert-base-turkish-uncased':\n",
        "    MODEL_PATH = MODEL_NAME\n",
        "    TOKENIZER_PATH = MODEL_NAME\n",
        "else:\n",
        "    MODEL_PATH = PROJECT_DIR + '/' + MODEL_NAME\n",
        "    TOKENIZER_PATH = PROJECT_DIR + '/' + MODEL_NAME\n",
        "\n",
        "DATA_DIR = PROJECT_DIR + '/data'\n",
        "DATASET_FILE = DATA_DIR + '/UserStory_1007.csv'\n",
        "TRAINING_DATA_TYPE = \"Cosmic Point\"\n",
        "TARGET_COL = 'cosmic_exit'\n",
        "INPUT_COL = 'sample_description' # 'sample_description' or 'TEXT_TR'\n",
        "SAVE_METRIC = 'ACCURACY' # models will be saved based on this metric (ACCURACY or PRED30 or MSE)\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8E9hi517NaeA"
      },
      "source": [
        "**Initialize the CUDA device**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBRMJo4xNZLX",
        "outputId": "7a3146cd-2a4c-45ac-9394-3f682af8dda5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of available GPUs: 1\n",
            "Active GPU: NVIDIA RTX 6000 Ada Generation\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "  print('Number of available GPUs: {}'.format(torch.cuda.device_count()))\n",
        "  print('Active GPU: {}'.format(torch.cuda.get_device_name(0)))\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "  print('GPU resource was not found. Using CPU resource.')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8fv1MHNSLC6G"
      },
      "source": [
        "# **Dataset Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "2CB0IdWrLPII",
        "outputId": "3ad003eb-2209-4941-910e-5729ff74d3d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Samples: 1007\n",
            "Total Nan Values in TARGET_COL: 0\n",
            "Total Nan Values in TARGET_COL: 0\n",
            "Total Valid Samples: 1007\n",
            "Number of Unique Labels: 4\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sample_description</th>\n",
              "      <th>cosmic_exit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>As a anonymoususer, I want to view a list of s...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>As a anonymoususer, I want to view a list of u...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>As a trainingcoordinator, I want to email all ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>As a attendee, I want to have a very clear map...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>As a trainer, I want to edit my training node ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1002</th>\n",
              "      <td>As a website user, I want to see updated finan...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1003</th>\n",
              "      <td>As a stakeholder, I want to see the results of...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1004</th>\n",
              "      <td>As a team member, I want to have a prioritized...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1005</th>\n",
              "      <td>As a content editor, I want to be able to easi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1006</th>\n",
              "      <td>As a user, I want to add a video to my article.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1007 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     sample_description  cosmic_exit\n",
              "0     As a anonymoususer, I want to view a list of s...            1\n",
              "1     As a anonymoususer, I want to view a list of u...            1\n",
              "2     As a trainingcoordinator, I want to email all ...            1\n",
              "3     As a attendee, I want to have a very clear map...            1\n",
              "4     As a trainer, I want to edit my training node ...            0\n",
              "...                                                 ...          ...\n",
              "1002  As a website user, I want to see updated finan...            1\n",
              "1003  As a stakeholder, I want to see the results of...            1\n",
              "1004  As a team member, I want to have a prioritized...            1\n",
              "1005  As a content editor, I want to be able to easi...            1\n",
              "1006    As a user, I want to add a video to my article.            2\n",
              "\n",
              "[1007 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load dataset file as pandas dataframe\n",
        "dataset = pd.read_csv(DATASET_FILE, delimiter=',')\n",
        "print(\"Total Samples: {}\".format(dataset.shape[0]))\n",
        "\n",
        "# Count Nan values in TARGET_COL\n",
        "print(\"Total Nan Values in TARGET_COL: {}\".format(dataset[TARGET_COL].isnull().sum()))\n",
        "# Fill Nan values in TARGET_COL with 0\n",
        "dataset[TARGET_COL] = dataset[TARGET_COL].fillna(0)\n",
        "print(\"Total Nan Values in TARGET_COL: {}\".format(dataset[TARGET_COL].isnull().sum()))\n",
        "\n",
        "dataset = dataset[[INPUT_COL, TARGET_COL]]\n",
        "dataset = dataset.dropna(axis=0, subset=[TARGET_COL]).reset_index(drop=True)\n",
        "dataset[TARGET_COL] = dataset[TARGET_COL].astype(int)\n",
        "print(\"Total Valid Samples: {}\".format(len(dataset)))\n",
        "\n",
        "NUM_LABELS = len(dataset[TARGET_COL].unique())\n",
        "print(\"Number of Unique Labels: {}\".format(NUM_LABELS))\n",
        "display(dataset)\n",
        "\n",
        "result_dataset = dataset.copy()\n",
        "result_dataset[\"prediction\"] = None\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qeAomdYfmXVJ"
      },
      "source": [
        "**Split the description and measurement columns for processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAkAWGOXmkp5",
        "outputId": "821f8399-77b3-450f-9b72-3da0eb5980b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculate MRE = False\n"
          ]
        }
      ],
      "source": [
        "inputs = dataset[INPUT_COL].values.astype(\"str\")\n",
        "# Convert line breaks to the spaces\n",
        "inputs = [input.strip().replace(\"\\n\",\" \") for input in inputs]\n",
        "targets = dataset[TARGET_COL].values\n",
        "CALCULATE_MRE = all(dataset[TARGET_COL] > 0)\n",
        "print(\"Calculate MRE =\", CALCULATE_MRE)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ac0H-jXegvW8"
      },
      "source": [
        "# **Model Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYz2qzW3gzvF",
        "outputId": "50c80f40-4e14-41ac-b62b-f76d7427051b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def initialize_model():\n",
        "    return BertForSequenceClassification.from_pretrained(MODEL_PATH, num_labels = 1)\n",
        "\n",
        "model = initialize_model()\n",
        "model.to(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ6FS0ypnRKT"
      },
      "source": [
        "**Encode samples and create a TensorDataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dJk5exUoJnY",
        "outputId": "1e47d396-f275-4e51-94d9-68efa764fc61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max input length: 95\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(TOKENIZER_PATH)\n",
        "\n",
        "# Find the max sample length\n",
        "max_input_length = 0\n",
        "for input in inputs:\n",
        "  input_ids = tokenizer.encode(input, add_special_tokens=True)\n",
        "  max_input_length = max(max_input_length, len(input_ids))\n",
        "\n",
        "print(\"Max input length: {}\".format(max_input_length))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "y4ThWxPMraLD"
      },
      "outputs": [],
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for input in inputs:\n",
        "  encoded_input = tokenizer.encode_plus(input,\n",
        "                                         add_special_tokens=True,\n",
        "                                         max_length=max_input_length,\n",
        "                                         truncation=True,\n",
        "                                         padding=\"max_length\",\n",
        "                                         return_attention_mask=True,\n",
        "                                         return_tensors='pt',)\n",
        "  input_ids.append(encoded_input[\"input_ids\"])\n",
        "  attention_masks.append(encoded_input[\"attention_mask\"])\n",
        "\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(targets, dtype=torch.float)\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qGvgHXsF0qZT"
      },
      "source": [
        "# **Training & Evaluation Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qcdwwtl-0-5-"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, scheduler, train_loader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        model.zero_grad()\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "        output = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        total_loss += output.loss.item()\n",
        "        output.loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def evaluate(model, data_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0\n",
        "    label_data = []\n",
        "    prediction_data = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch[0].to(device)\n",
        "            attention_mask = batch[1].to(device)\n",
        "            labels = batch[2].to(device)\n",
        "            output = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = criterion(output.logits.squeeze(-1), labels.float())\n",
        "            total_loss += loss.item()\n",
        "            logits = output.logits.to('cpu').numpy()\n",
        "            label_ids = labels.to('cpu').numpy()\n",
        "            label_data += label_ids.tolist()\n",
        "            prediction_data += logits.flatten().tolist()\n",
        "\n",
        "    return label_data, prediction_data, total_loss / len(data_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCIhNSucpLEU",
        "outputId": "cbd1fbe8-4db2-4bb8-8eda-4662dbe603c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "./results/SE-BERT-UserStory_1007-cosmic_exit\n"
          ]
        }
      ],
      "source": [
        "folder_name = MODEL_NAME + \"-\" + DATASET_FILE.split(\"/\")[-1].split(\".\")[0] + \"-\" + TARGET_COL\n",
        "results_folder = PROJECT_DIR + \"/results/\" + folder_name\n",
        "print(results_folder)\n",
        "%mkdir -p {results_folder}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "juUJkrcb1TdX"
      },
      "source": [
        "# **Run Cross-Validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Xd67-Ero1oaI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              Model Name: SE-BERT\n",
            "          Training Scope: Cosmic Point\n",
            "        Number of Splits: 5\n",
            "                 Shuffle: True\n",
            "\n",
            "                    FOLD: 1\n",
            "\n",
            "Epoch 01:  train_loss=0.2019  test_loss=0.1390  mae=0.2314  nmae=0.2077  mse=0.1337  acc=0.8713\n",
            "New overall max accuracy (0.8713). Saving as the best model...\n",
            "Epoch 02:  train_loss=0.1093  test_loss=0.1382  mae=0.2421  nmae=0.2173  mse=0.1321  acc=0.8614\n",
            "Epoch 03:  train_loss=0.0777  test_loss=0.1349  mae=0.2126  nmae=0.1908  mse=0.1259  acc=0.8762\n",
            "New overall max accuracy (0.8762). Saving as the best model...\n",
            "Epoch 04:  train_loss=0.0566  test_loss=0.1488  mae=0.2675  nmae=0.2402  mse=0.1433  acc=0.8317\n",
            "Epoch 05:  train_loss=0.0345  test_loss=0.1666  mae=0.2598  nmae=0.2332  mse=0.1622  acc=0.8168\n",
            "Epoch 06:  train_loss=0.0243  test_loss=0.1509  mae=0.2305  nmae=0.2069  mse=0.1441  acc=0.8416\n",
            "Epoch 07:  train_loss=0.0166  test_loss=0.1376  mae=0.2047  nmae=0.1838  mse=0.1276  acc=0.8663\n",
            "Epoch 08:  train_loss=0.0139  test_loss=0.1396  mae=0.2091  nmae=0.1877  mse=0.1291  acc=0.8663\n",
            "Epoch 09:  train_loss=0.0104  test_loss=0.1294  mae=0.1956  nmae=0.1756  mse=0.1198  acc=0.8812\n",
            "New overall max accuracy (0.8812). Saving as the best model...\n",
            "Epoch 10:  train_loss=0.0113  test_loss=0.1309  mae=0.2021  nmae=0.1815  mse=0.1217  acc=0.8762\n",
            "\n",
            "                    FOLD: 2\n",
            "\n",
            "Epoch 01:  train_loss=0.1852  test_loss=0.1081  mae=0.2646  nmae=0.2355  mse=0.1171  acc=0.8812\n",
            "Epoch 02:  train_loss=0.1296  test_loss=0.1330  mae=0.3451  nmae=0.3071  mse=0.1411  acc=0.8861\n",
            "New overall max accuracy (0.8861). Saving as the best model...\n",
            "Epoch 03:  train_loss=0.1104  test_loss=0.0898  mae=0.1814  nmae=0.1614  mse=0.0994  acc=0.8861\n",
            "Epoch 04:  train_loss=0.0719  test_loss=0.1036  mae=0.1890  nmae=0.1682  mse=0.1137  acc=0.8861\n",
            "Epoch 05:  train_loss=0.0447  test_loss=0.1180  mae=0.2346  nmae=0.2087  mse=0.1295  acc=0.8366\n",
            "Epoch 06:  train_loss=0.0261  test_loss=0.0990  mae=0.2043  nmae=0.1818  mse=0.1086  acc=0.8564\n",
            "Epoch 07:  train_loss=0.0176  test_loss=0.1002  mae=0.2065  nmae=0.1838  mse=0.1099  acc=0.8812\n",
            "Epoch 08:  train_loss=0.0131  test_loss=0.0954  mae=0.1822  nmae=0.1621  mse=0.1050  acc=0.8762\n",
            "Epoch 09:  train_loss=0.0111  test_loss=0.0974  mae=0.1861  nmae=0.1656  mse=0.1072  acc=0.8762\n",
            "Epoch 10:  train_loss=0.0097  test_loss=0.0976  mae=0.1935  nmae=0.1722  mse=0.1072  acc=0.8762\n",
            "\n",
            "                    FOLD: 3\n",
            "\n",
            "Epoch 01:  train_loss=0.2211  test_loss=0.1263  mae=0.2488  nmae=0.2253  mse=0.1229  acc=0.9154\n",
            "New overall max accuracy (0.9154). Saving as the best model...\n",
            "Epoch 02:  train_loss=0.1188  test_loss=0.1253  mae=0.2483  nmae=0.2248  mse=0.1226  acc=0.9154\n",
            "Epoch 03:  train_loss=0.0963  test_loss=0.1127  mae=0.1890  nmae=0.1711  mse=0.1097  acc=0.9104\n",
            "Epoch 04:  train_loss=0.0647  test_loss=0.1063  mae=0.1766  nmae=0.1599  mse=0.1043  acc=0.9055\n",
            "Epoch 05:  train_loss=0.0396  test_loss=0.1185  mae=0.1721  nmae=0.1558  mse=0.1184  acc=0.8955\n",
            "Epoch 06:  train_loss=0.0279  test_loss=0.1113  mae=0.1765  nmae=0.1598  mse=0.1104  acc=0.8905\n",
            "Epoch 07:  train_loss=0.0161  test_loss=0.1103  mae=0.1772  nmae=0.1605  mse=0.1098  acc=0.8756\n",
            "Epoch 08:  train_loss=0.0150  test_loss=0.1069  mae=0.1670  nmae=0.1512  mse=0.1063  acc=0.8806\n",
            "Epoch 09:  train_loss=0.0115  test_loss=0.1106  mae=0.1686  nmae=0.1526  mse=0.1104  acc=0.8856\n",
            "Epoch 10:  train_loss=0.0113  test_loss=0.1103  mae=0.1670  nmae=0.1512  mse=0.1100  acc=0.8905\n",
            "\n",
            "                    FOLD: 4\n",
            "\n",
            "Epoch 01:  train_loss=0.1759  test_loss=0.1451  mae=0.2577  nmae=0.2272  mse=0.1490  acc=0.8408\n",
            "Epoch 02:  train_loss=0.1110  test_loss=0.1375  mae=0.2580  nmae=0.2274  mse=0.1390  acc=0.8607\n",
            "Epoch 03:  train_loss=0.0947  test_loss=0.1320  mae=0.2126  nmae=0.1875  mse=0.1323  acc=0.8607\n",
            "Epoch 04:  train_loss=0.0633  test_loss=0.1288  mae=0.2448  nmae=0.2158  mse=0.1282  acc=0.8607\n",
            "Epoch 05:  train_loss=0.0423  test_loss=0.1452  mae=0.2860  nmae=0.2521  mse=0.1449  acc=0.8408\n",
            "Epoch 06:  train_loss=0.0261  test_loss=0.1233  mae=0.1981  nmae=0.1746  mse=0.1244  acc=0.8607\n",
            "Epoch 07:  train_loss=0.0162  test_loss=0.1238  mae=0.1960  nmae=0.1727  mse=0.1249  acc=0.8607\n",
            "Epoch 08:  train_loss=0.0135  test_loss=0.1329  mae=0.2152  nmae=0.1897  mse=0.1333  acc=0.8607\n",
            "Epoch 09:  train_loss=0.0096  test_loss=0.1288  mae=0.1974  nmae=0.1741  mse=0.1301  acc=0.8607\n",
            "Epoch 10:  train_loss=0.0087  test_loss=0.1261  mae=0.1998  nmae=0.1762  mse=0.1270  acc=0.8607\n",
            "\n",
            "                    FOLD: 5\n",
            "\n",
            "Epoch 01:  train_loss=0.2199  test_loss=0.1043  mae=0.1628  nmae=0.1442  mse=0.1161  acc=0.8706\n",
            "Epoch 02:  train_loss=0.1317  test_loss=0.0948  mae=0.2526  nmae=0.2237  mse=0.1043  acc=0.8756\n",
            "Epoch 03:  train_loss=0.1045  test_loss=0.0944  mae=0.2561  nmae=0.2267  mse=0.1040  acc=0.8706\n",
            "Epoch 04:  train_loss=0.0768  test_loss=0.0998  mae=0.2394  nmae=0.2119  mse=0.1085  acc=0.8706\n",
            "Epoch 05:  train_loss=0.0453  test_loss=0.0959  mae=0.2178  nmae=0.1928  mse=0.1051  acc=0.8856\n",
            "Epoch 06:  train_loss=0.0193  test_loss=0.0898  mae=0.1723  nmae=0.1526  mse=0.0995  acc=0.8756\n",
            "Epoch 07:  train_loss=0.0139  test_loss=0.0892  mae=0.1728  nmae=0.1530  mse=0.0988  acc=0.8706\n",
            "Epoch 08:  train_loss=0.0109  test_loss=0.0974  mae=0.1847  nmae=0.1636  mse=0.1075  acc=0.8657\n",
            "Epoch 09:  train_loss=0.0107  test_loss=0.0892  mae=0.1686  nmae=0.1493  mse=0.0988  acc=0.8657\n",
            "Epoch 10:  train_loss=0.0099  test_loss=0.0911  mae=0.1674  nmae=0.1482  mse=0.1010  acc=0.8657\n"
          ]
        }
      ],
      "source": [
        "kf = KFold(n_splits=N_SPLITS, random_state=SEED, shuffle=KFOLD_SHUFFLE)\n",
        "\n",
        "out = \"\"\n",
        "\n",
        "model_name = \"              Model Name: {}\".format(MODEL_NAME)\n",
        "out += model_name+\"\\n\"\n",
        "print(model_name)\n",
        "\n",
        "training_scope = \"          Training Scope: {}\".format(TRAINING_DATA_TYPE)\n",
        "out += training_scope+\"\\n\"\n",
        "print(training_scope)\n",
        "\n",
        "number_of_splits = \"        Number of Splits: {}\".format(N_SPLITS)\n",
        "out += number_of_splits+\"\\n\"\n",
        "print(number_of_splits)\n",
        "\n",
        "shuffle = \"                 Shuffle: {}\".format(KFOLD_SHUFFLE)\n",
        "out += shuffle+\"\\n\"\n",
        "print(shuffle)\n",
        "\n",
        "pred30s = []\n",
        "mres = []\n",
        "mses = []\n",
        "maes = []\n",
        "nmaes = []\n",
        "\n",
        "overall_min_mse = 999999\n",
        "overall_max_pred30 = -1\n",
        "overall_max_accuracy = -1\n",
        "overall_best_model = None\n",
        "\n",
        "for fold, (train_index, test_index) in enumerate(kf.split(dataset)):\n",
        "  # RESET WEIGHTS IN EACH FOLD:\n",
        "  model = initialize_model().to(device)\n",
        "  criterion = torch.nn.MSELoss()\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, eps=1e-8)\n",
        "\n",
        "  # GENERATE DATALOADERS IN EACH FOLD:\n",
        "  train_subset = torch.utils.data.Subset(dataset, train_index)\n",
        "  test_subset = torch.utils.data.Subset(dataset, test_index)\n",
        "  train_dataloader = torch.utils.data.DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "  test_dataloader = torch.utils.data.DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "  total_steps = len(train_dataloader) * NUM_EPOCHS\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "  fold_text = \"\\n                    FOLD: {}\\n\".format(fold+1)\n",
        "  out += fold_text + \"\\n\"\n",
        "  print(fold_text)\n",
        "\n",
        "  fold_min_mse = 999999\n",
        "  fold_max_pred30 = -1\n",
        "  fold_max_accuracy = -1\n",
        "  results = []\n",
        "\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "      train_loss = train(model, optimizer, scheduler, train_dataloader)\n",
        "      labels, preds, test_loss = evaluate(model, test_dataloader, criterion)\n",
        "\n",
        "      result = pd.DataFrame.from_dict({\"label\": labels, \"prediction\": preds})\n",
        "      ses = [math.pow(preds[i]-labels[i], 2) for i in range(len(preds))]\n",
        "      aes = [np.abs(preds[i]-labels[i]) for i in range(len(preds))]\n",
        "      mse = np.mean(ses)\n",
        "      mae = np.mean(aes)\n",
        "      nmae = mae / np.mean(labels)\n",
        "      rounded_preds = [round(pred) for pred in preds]\n",
        "      accuracy = sum([1 for i in range(len(preds)) if rounded_preds[i] == labels[i]]) / len(preds)\n",
        "      result = {\"epoch\": epoch+1, \"train_loss\": train_loss, \"test_loss\": test_loss, \"mse\": mse, \"mae\": mae, \"nmae\": nmae, \"accuracy\": accuracy}\n",
        "      epoch_text = f\"Epoch {epoch+1:02d}:  train_loss={train_loss:.4f}  test_loss={test_loss:.4f}  mae={mae:.4f}  nmae={nmae:.4f}  mse={mse:.4f}  acc={accuracy:.4f}\"\n",
        "\n",
        "\n",
        "      if CALCULATE_MRE:\n",
        "        res = [(np.abs(preds[i]-labels[i]) / labels[i]) for i in range(len(preds))]\n",
        "        resUnder30 = [re for re in res if re < 0.3]\n",
        "        pred30 = len(resUnder30)/len(res)\n",
        "        mre = np.mean(res)\n",
        "        result[\"mre\"] = mre\n",
        "        result[\"pred30\"] = pred30\n",
        "        epoch_text += f\"  mre={mre:.4f}  pred30={pred30:.4f}\"\n",
        "\n",
        "      print(epoch_text)\n",
        "      if CALCULATE_MRE and SAVE_METRIC == \"PRED30\":\n",
        "        # save model based on PRED30\n",
        "        if pred30 > fold_max_pred30:\n",
        "          # use the predictions of this epoch\n",
        "          fold_max_pred30 = pred30\n",
        "          for i, index in enumerate(test_index):\n",
        "            result_dataset.at[index, \"prediction\"] = preds[i]\n",
        "\n",
        "        if pred30 > overall_max_pred30:\n",
        "          # select this epoch as the best model\n",
        "          overall_max_pred30 = pred30\n",
        "          overall_best_model = copy.deepcopy(model)\n",
        "          print(f\"New overall max PRED30 ({pred30:.4f}). Saving as the best model...\")\n",
        "\n",
        "      elif SAVE_METRIC == \"MSE\":\n",
        "        # save model based on MSE\n",
        "        if mse < fold_min_mse:\n",
        "          fold_min_mse = mse\n",
        "          for i, index in enumerate(test_index):\n",
        "            result_dataset.at[index, \"prediction\"] = preds[i]\n",
        "        if test_loss < overall_min_mse:\n",
        "          overall_min_mse = test_loss\n",
        "          overall_best_model = copy.deepcopy(model)\n",
        "          print(f\"New overall min MSE ({mse:.4f}). Saving as the best model...\")\n",
        "      \n",
        "      elif SAVE_METRIC == \"ACCURACY\":\n",
        "        # save model based on accuracy\n",
        "        if accuracy > fold_max_accuracy:\n",
        "          fold_max_accuracy = accuracy\n",
        "          for i, index in enumerate(test_index):\n",
        "            result_dataset.at[index, \"prediction\"] = preds[i]\n",
        "        if accuracy > overall_max_accuracy:\n",
        "          overall_max_accuracy = accuracy\n",
        "          overall_best_model = copy.deepcopy(model)\n",
        "          print(f\"New overall max accuracy ({accuracy:.4f}). Saving as the best model...\")\n",
        "\n",
        "      results.append(result)\n",
        "      out += epoch_text + \"\\n\"\n",
        "\n",
        "  if CALCULATE_MRE:\n",
        "    max_pred30 = 0\n",
        "    max_index = -1\n",
        "    for i, result in enumerate(results):\n",
        "      if result[\"pred30\"] > max_pred30:\n",
        "        max_pred30 = result[\"pred30\"]\n",
        "        max_index = i\n",
        "\n",
        "    pred30s.append(results[max_index][\"pred30\"])\n",
        "    mres.append(results[max_index][\"mre\"])\n",
        "    mses.append(results[max_index][\"mse\"])\n",
        "    maes.append(results[max_index][\"mae\"])\n",
        "    nmaes.append(results[max_index][\"nmae\"])\n",
        "  else:\n",
        "    min_mse = 0\n",
        "    min_index = -1\n",
        "    for i, result in enumerate(results):\n",
        "      if result[\"mse\"] < min_mse:\n",
        "        min_mse = result[\"mse\"]\n",
        "        min_index = i\n",
        "    mses.append(results[min_index][\"mse\"])\n",
        "    maes.append(results[min_index][\"mae\"])\n",
        "    nmaes.append(results[min_index][\"nmae\"])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PRINT AVERAGES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "         Model: SE-BERT\n",
            "        Target: cosmic_exit\n",
            "  Dataset Size: 1007\n",
            "  Num of Folds: 5\n",
            "   Average MAE: 0.1860\n",
            "  Average NMAE: 0.1658\n",
            "   Average MSE: 0.1134\n",
            "   Average ACC: 0.8858\n"
          ]
        }
      ],
      "source": [
        "out_footer = \"\"\n",
        "print()\n",
        "\n",
        "mae_text= f\"   Average MAE: {np.mean(maes):.4f}\"\n",
        "out_footer += mae_text+\"\\n\"\n",
        "nmae_text= f\"  Average NMAE: {np.mean(nmaes):.4f}\"\n",
        "out_footer += nmae_text+\"\\n\"\n",
        "mse_text=  f\"   Average MSE: {np.mean(mses):.4f}\"\n",
        "out_footer += mse_text+\"\\n\"\n",
        "\n",
        "if CALCULATE_MRE:\n",
        "  mre_text= f\"   Average MRE: {np.mean(mres):.4f}\"\n",
        "  out_footer += mre_text+\"\\n\"\n",
        "  pred30_text= f\"Average PRED30: {np.mean(pred30s):.4f}\"\n",
        "  out_footer += pred30_text+\"\\n\"\n",
        "\n",
        "predictions = result_dataset[\"prediction\"].values\n",
        "rounded_predictions = [round(pred) for pred in predictions]\n",
        "actuals = result_dataset[TARGET_COL].values\n",
        "accuracy = np.mean(rounded_predictions == actuals)\n",
        "accuracy_text = f\"   Average ACC: {accuracy:.4f}\"\n",
        "out_footer += accuracy_text\n",
        "\n",
        "print(\"         Model:\", MODEL_NAME)\n",
        "print(\"        Target:\", TARGET_COL)\n",
        "print(\"  Dataset Size:\", len(dataset))\n",
        "print(\"  Num of Folds:\", N_SPLITS)\n",
        "print(out_footer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "96v2VYz_jI7s"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RUN FINISHED...\n"
          ]
        }
      ],
      "source": [
        "f = open(results_folder+\"/result.txt\", \"w+\")\n",
        "f.write(out + out_footer)\n",
        "f.close()\n",
        "print(\"RUN FINISHED...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hPJFOjB2bZ8A"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PREDICTIONS SAVED FINISHED...\n",
            "MODEL SAVED FINISHED...\n"
          ]
        }
      ],
      "source": [
        "result_dataset.to_csv(results_folder + \"/predictions.csv\", index=False)\n",
        "print(\"PREDICTIONS SAVED FINISHED...\")\n",
        "overall_best_model.save_pretrained(results_folder + \"/{}_{:.4f}\".format(folder_name, accuracy))\n",
        "print(\"MODEL SAVED FINISHED...\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Merge all predictions.csv under /results/*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import os\n",
        "# import pandas as pd\n",
        "# PROJECT_DIR = '.'\n",
        "# results_folder = PROJECT_DIR + \"/results\"\n",
        "# folders = os.listdir(results_folder)\n",
        "# folders = [folder for folder in folders if folder.startswith(\"BERT\") or folder.startswith(\"bert\") or folder.startswith(\"SE-BERT\")]\n",
        "# folders = sorted(folders)\n",
        "# for f in folders:\n",
        "#     print(f)\n",
        "\n",
        "# dfs = []\n",
        "# for folder in folders:\n",
        "#   folder_path = results_folder + \"/\" + folder\n",
        "#   prediction_file = folder_path + \"/predictions.csv\"\n",
        "#   df = pd.read_csv(prediction_file)\n",
        "#   # get only the predictions column\n",
        "#   df = df.iloc[:,2]\n",
        "#   dfs.append(df)\n",
        "\n",
        "# new_col_names = []\n",
        "# for f in folders:\n",
        "#     new_col_names.append(f.split(\"UserStory_1007-\")[0] + f.split(\"UserStory_1007-\")[1])\n",
        "\n",
        "# merged_df = pd.concat(dfs, axis=1)\n",
        "# merged_df.columns = new_col_names\n",
        "# merged_df.to_csv(results_folder + \"/merged_predictions.csv\", index=False)\n",
        "# print(\"MERGED PREDICTIONS SAVED FINISHED...\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
